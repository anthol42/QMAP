{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# How to use the benchmark\n",
    "There are 5 different test splits in the benchmark. A model should be tested on each split to evaluate its performances accurately. The means that the model must be trained five times on different training and validation sets that are separated from the current test split. The hyperparameters of the model should stay the same for all splits. This notebook will show a simple example on how to use the benchmark."
   ],
   "id": "d7ad3a94ed894405"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T16:00:20.735482Z",
     "start_time": "2026-01-26T16:00:10.739370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Utility function for our model in the example. This is not part of QMAP.\n",
    "import torch\n",
    "from transformers import EsmTokenizer, EsmModel\n",
    "import numpy as np\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "\n",
    "def generate_esm2_embeddings(protein_sequences: List[str],\n",
    "                             model_name: str = \"facebook/esm2_t33_650M_UR50D\",\n",
    "                             device: str = None,\n",
    "                             batch_size: int = 64) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generate protein embeddings using ESM2 model with mean pooling.\n",
    "\n",
    "    Args:\n",
    "        protein_sequences (List[str]): List of protein sequences (amino acid sequences)\n",
    "        model_name (str): Name of the ESM2 model to use. Options include:\n",
    "            - \"facebook/esm2_t6_8M_UR50D\" (8M parameters)\n",
    "            - \"facebook/esm2_t12_35M_UR50D\" (35M parameters)\n",
    "            - \"facebook/esm2_t30_150M_UR50D\" (150M parameters)\n",
    "            - \"facebook/esm2_t33_650M_UR50D\" (650M parameters) - default\n",
    "            - \"facebook/esm2_t36_3B_UR50D\" (3B parameters)\n",
    "            - \"facebook/esm2_t48_15B_UR50D\" (15B parameters)\n",
    "        device (str): Device to run the model on ('cuda', 'cpu', or None for auto-detection)\n",
    "        batch_size (int): Batch size for processing sequences\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of shape (num_sequences, embedding_dim) containing embeddings\n",
    "                   in the same order as input sequences\n",
    "    \"\"\"\n",
    "\n",
    "    # Set device\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    # Load tokenizer and model\n",
    "    tokenizer = EsmTokenizer.from_pretrained(model_name)\n",
    "    model = EsmModel.from_pretrained(model_name)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    # Process sequences in batches\n",
    "    for i in tqdm(range(0, len(protein_sequences), batch_size)):\n",
    "        batch_sequences = protein_sequences[i:i + batch_size]\n",
    "\n",
    "        # Tokenize sequences\n",
    "        inputs = tokenizer(batch_sequences,\n",
    "                           return_tensors=\"pt\",\n",
    "                           padding=True,\n",
    "                           truncation=True,\n",
    "                           max_length=1024)\n",
    "\n",
    "        # Move to device\n",
    "        inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "        # Generate embeddings\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            hidden_states = outputs.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # Process each sequence in the batch\n",
    "        for j in range(len(batch_sequences)):\n",
    "            # Get attention mask for this sequence (to exclude padding tokens)\n",
    "            attention_mask = inputs['attention_mask'][j]\n",
    "            seq_embeddings = hidden_states[j][attention_mask.bool()]  # Remove padding\n",
    "\n",
    "            # Remove special tokens (CLS and EOS tokens)\n",
    "            seq_embeddings_no_special = seq_embeddings[1:-1]  # Remove first (CLS) and last (EOS) tokens\n",
    "\n",
    "            # Mean pooling\n",
    "            seq_embedding = seq_embeddings_no_special.mean(dim=0)\n",
    "            all_embeddings.append(seq_embedding.cpu().numpy())\n",
    "\n",
    "    return np.array(all_embeddings)"
   ],
   "id": "c280204e4ce6e46f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonylavertu/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/anthonylavertu/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MIC",
   "id": "6c9863c13d47d2a9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T17:31:58.747574Z",
     "start_time": "2026-01-26T17:20:03.197577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qmap.benchmark import QMAPBenchmark, DBAASPDataset\n",
    "\n",
    "# Imports for the example\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Load the training dataset.\n",
    "# For this example, we will load the DBAASP dataset that is packaged with QMAP. We want to keep only samples that have a target for Escherichia coli.\n",
    "dataset = (DBAASPDataset()\n",
    "           .with_bacterial_targets(['Escherichia coli'])\n",
    "           )\n",
    "\n",
    "\n",
    "# Loop over all splits of the benchmark\n",
    "for split in range(5):\n",
    "    # Step 2: Load the benchmark.\n",
    "    # For this example, we will only consider the specie Escherichia coli. So we filter out any sample that does not have a target for this specie.\n",
    "    benchmark = QMAPBenchmark(split).with_bacterial_targets(['Escherichia coli'])\n",
    "    # Step 3: Get the training mask. This means the sequences that can be used to train the model because they are dissimilar enough from the test set.\n",
    "    train_mask = benchmark.get_train_mask([seq for seq in dataset.sequences])\n",
    "    # Step 4: Filter the dataset to only keep the sequences that can be used for training\n",
    "    train_dataset = dataset[train_mask]\n",
    "\n",
    "    # Step 5: Train the model on the training dataset\n",
    "    # For this example, we will use a linear model on top of the encoder's embeddings\n",
    "    y_train = np.log10(np.array([sample.targets['Escherichia coli'].consensus for sample in train_dataset]))\n",
    "    X_train = generate_esm2_embeddings(train_dataset.sequences)\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Step 6: Get the test sequences and their targets\n",
    "    # The only inputs we have are the sequences because we turned off all options. However, if you want to include C and N terminal modifications, you can do this in the benchmark constructor. When accessing the inputs attribute, you will receive the sequences, the C terminal modifications, and the N terminal modifications. The same concept applies for unusual amino acids or species as inputs which will include the specie name with the sequence.\n",
    "    test_sequences = benchmark.sequences\n",
    "\n",
    "    # Step 7: Get the predictions of the model on the test set\n",
    "    X_test = generate_esm2_embeddings(test_sequences)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(y_pred.shape)\n",
    "\n",
    "    # Conver the predictions to a format compatible with the benchmark evaluation (List of dictionaries, where each key is a specie name or HC50)\n",
    "    y_pred = [{'Escherichia coli': pred} for pred in y_pred]\n",
    "\n",
    "    # Step 8: Evaluate the model\n",
    "    # The benchmark provides a method to evaluate the model on the test set given the predictions.\n",
    "    results = benchmark.compute_metrics(y_pred)['Escherichia coli']\n",
    "\n",
    "    # You can also get the results as a dictionary\n",
    "    results_dict = results.dict()"
   ],
   "id": "82f3e0d691e485c1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded binary mask from cache: /Users/anthonylavertu/Library/Caches/pwiden_engine/binary_mask_e896fd46047b26e442355e893b5e544d36f44d7e940d412893cc4b8ac9c6945d_thresh_0.6000.npy\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 134/134 [08:27<00:00,  3.79s/it]\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "100%|██████████| 33/33 [03:00<00:00,  5.46s/it]\n",
      "/Users/anthonylavertu/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: divide by zero encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/anthonylavertu/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: overflow encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n",
      "/Users/anthonylavertu/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/sklearn/linear_model/_base.py:279: RuntimeWarning: invalid value encountered in matmul\n",
      "  return X @ coef_ + self.intercept_\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2112,)\n",
      "QMAPMetrics(property: Escherichia coli; 2112):\n",
      " - RMSE: 0.7276\n",
      " - MSE: 0.5295\n",
      " - MAE: 0.5682\n",
      " - R2: 0.0711\n",
      " - Spearman: 0.4281\n",
      " - Kendall's Tau: 0.2973\n",
      " - Pearson: 0.4563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saved binary mask to cache: /Users/anthonylavertu/Library/Caches/pwiden_engine/binary_mask_887b7ab0e9a9db89de59c11616da9d7ac7e3e8f2161a20dee8f77f9a2b47cf9f_thresh_0.6000.npy\n",
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  3%|▎         | 4/130 [00:14<07:36,  3.63s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 27\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;66;03m# Step 5: Train the model on the training dataset\u001B[39;00m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# For this example, we will use a linear model on top of the encoder's embeddings\u001B[39;00m\n\u001B[1;32m     26\u001B[0m y_train \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mlog10(np\u001B[38;5;241m.\u001B[39marray([sample\u001B[38;5;241m.\u001B[39mtargets[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mEscherichia coli\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mconsensus \u001B[38;5;28;01mfor\u001B[39;00m sample \u001B[38;5;129;01min\u001B[39;00m train_dataset]))\n\u001B[0;32m---> 27\u001B[0m X_train \u001B[38;5;241m=\u001B[39m \u001B[43mgenerate_esm2_embeddings\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msequences\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     28\u001B[0m model \u001B[38;5;241m=\u001B[39m LinearRegression()\n\u001B[1;32m     29\u001B[0m model\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n",
      "Cell \u001B[0;32mIn[1], line 60\u001B[0m, in \u001B[0;36mgenerate_esm2_embeddings\u001B[0;34m(protein_sequences, model_name, device, batch_size)\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[38;5;66;03m# Generate embeddings\u001B[39;00m\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 60\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m outputs\u001B[38;5;241m.\u001B[39mlast_hidden_state  \u001B[38;5;66;03m# Shape: (batch_size, seq_len, hidden_size)\u001B[39;00m\n\u001B[1;32m     63\u001B[0m \u001B[38;5;66;03m# Process each sequence in the batch\u001B[39;00m\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/transformers/utils/generic.py:1072\u001B[0m, in \u001B[0;36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1069\u001B[0m                 monkey_patched_layers\u001B[38;5;241m.\u001B[39mappend((module, original_forward))\n\u001B[1;32m   1071\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1072\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1073\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m original_exception:\n\u001B[1;32m   1074\u001B[0m     \u001B[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001B[39;00m\n\u001B[1;32m   1075\u001B[0m     \u001B[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001B[39;00m\n\u001B[1;32m   1076\u001B[0m     \u001B[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001B[39;00m\n\u001B[1;32m   1077\u001B[0m     kwargs_without_recordable \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;129;01min\u001B[39;00m recordable_keys}\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py:748\u001B[0m, in \u001B[0;36mEsmModel.forward\u001B[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m    741\u001B[0m \u001B[38;5;66;03m# Prepare head mask if needed\u001B[39;00m\n\u001B[1;32m    742\u001B[0m \u001B[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001B[39;00m\n\u001B[1;32m    743\u001B[0m \u001B[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001B[39;00m\n\u001B[1;32m    744\u001B[0m \u001B[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001B[39;00m\n\u001B[1;32m    745\u001B[0m \u001B[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001B[39;00m\n\u001B[1;32m    746\u001B[0m head_mask \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_head_mask(head_mask, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39mnum_hidden_layers)\n\u001B[0;32m--> 748\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    749\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    750\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    751\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    752\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    753\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_extended_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    754\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    755\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    756\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    757\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler(sequence_output) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpooler \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/transformers/utils/generic.py:918\u001B[0m, in \u001B[0;36mcan_return_tuple.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    916\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m return_dict_passed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    917\u001B[0m     return_dict \u001B[38;5;241m=\u001B[39m return_dict_passed\n\u001B[0;32m--> 918\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    919\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m return_dict \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(output, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[1;32m    920\u001B[0m     output \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mto_tuple()\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py:556\u001B[0m, in \u001B[0;36mEsmEncoder.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m    554\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, layer_module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer):\n\u001B[1;32m    555\u001B[0m     layer_head_mask \u001B[38;5;241m=\u001B[39m head_mask[i] \u001B[38;5;28;01mif\u001B[39;00m head_mask \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m--> 556\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    557\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    558\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    559\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    560\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    561\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    562\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    563\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    565\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39memb_layer_norm_after:\n\u001B[1;32m    566\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39memb_layer_norm_after(hidden_states)\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/transformers/modeling_layers.py:94\u001B[0m, in \u001B[0;36mGradientCheckpointingLayer.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     91\u001B[0m         logger\u001B[38;5;241m.\u001B[39mwarning_once(message)\n\u001B[1;32m     93\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(partial(\u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs), \u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m---> 94\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__call__\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py:526\u001B[0m, in \u001B[0;36mEsmLayer.forward\u001B[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, **kwargs)\u001B[0m\n\u001B[1;32m    512\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[1;32m    513\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIf `encoder_hidden_states` are passed, \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m has to be instantiated\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    514\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m with cross-attention layers by setting `config.add_cross_attention=True`\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    515\u001B[0m         )\n\u001B[1;32m    517\u001B[0m     attention_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcrossattention(\n\u001B[1;32m    518\u001B[0m         attention_output,\n\u001B[1;32m    519\u001B[0m         attention_mask\u001B[38;5;241m=\u001B[39mattention_mask,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    523\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    524\u001B[0m     )\n\u001B[0;32m--> 526\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfeed_forward_chunk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_output\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py:531\u001B[0m, in \u001B[0;36mEsmLayer.feed_forward_chunk\u001B[0;34m(self, attention_output)\u001B[0m\n\u001B[1;32m    529\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mfeed_forward_chunk\u001B[39m(\u001B[38;5;28mself\u001B[39m, attention_output):\n\u001B[1;32m    530\u001B[0m     attention_output_ln \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mLayerNorm(attention_output)\n\u001B[0;32m--> 531\u001B[0m     intermediate_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mintermediate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mattention_output_ln\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    532\u001B[0m     layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput(intermediate_output, attention_output)\n\u001B[1;32m    533\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m layer_output\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/transformers/models/esm/modeling_esm.py:460\u001B[0m, in \u001B[0;36mEsmIntermediate.forward\u001B[0;34m(self, hidden_states)\u001B[0m\n\u001B[1;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m--> 460\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    461\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m gelu(hidden_states)\n\u001B[1;32m    462\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m hidden_states\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1773\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1772\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1773\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1784\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1779\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1780\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1781\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1782\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1783\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1784\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1786\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1787\u001B[0m called_always_called_hooks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mset\u001B[39m()\n",
      "File \u001B[0;32m~/mac_docs/pycharmProjects/QMAP/QMAP/.venv/lib/python3.9/site-packages/torch/nn/modules/linear.py:125\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Subsets\n",
    "The benchmark can be filtered to evaluate the model on different environment conditions. This is done easily using the filters methods of the benchmark. There are multiple filter methods pre-implemented that starts with the `with_` prefix.\n",
    "\n",
    "For example, we can obtain the high-efficiency subset used in the paper as follows:"
   ],
   "id": "6e5e5ba6c293c592"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T17:32:08.981787Z",
     "start_time": "2026-01-26T17:32:08.977543Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This gets all samples that have an efficiency below 10 uM for any bacterias in its targets.\n",
    "high_eff = benchmark.with_efficiency_below(10.)\n",
    "\n",
    "# The high_eff object is of the same type as benchmark, so you can use all its methods to evaluate the model on this subset.\n",
    "# results_high_eff = high_eff.compute_metrics(...)\n",
    "print(high_eff)"
   ],
   "id": "4373192d4d690a60",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBAASP ID  Sequence                                 N Terminal   C Terminal   # Targets  HC50  \n",
      "3681       GLIDTIKNMALNAAKSAGVSVLNTLSCKLSKTC        Free         Free         3          nan\n",
      "7811       RFRRLRCKTRCRLKKI                         Free         AMD          5          nan\n",
      "6813       GIHKILKYGKPS                             Free         AMD          5          nan\n",
      "5744       NVTPATKPTPSKPGYCRVMDELILCPDPPLSKDLCKNDSD Free         Free         3          nan\n",
      "                                               ...\n",
      "7641       RPPQFTRAQWFAIQHISLNPPRSTIAMRAINNYRWRSKNQ Free         Free         6          9.50\n",
      "9073       fwGalakGalklipslfssfskkd                 Free         Free         4          10.00\n",
      "14831      FYPRPYRPPYLPDPRPFPRPLPAFGHEFRRH          Free         Free         3          nan\n",
      "2144       FFGHLFKLATKIIPSLFQ                       Free         Free         7          nan\n",
      "\n",
      "Total of 1508 samples\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Extending filters\n",
    "You can also create your own filter function by making a callback function that takes as input a `Sample` and returns a boolean indicating whether to keep the sample or not (True to keep). You can then use the `filter` method of the benchmark to create a new benchmark with only the samples that satisfy the condition.\n",
    "\n",
    "For example, here we create a filter to keep only samples that have a hemolytic activity (HC50) above 100 uM."
   ],
   "id": "95ecb1fba8311b19"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-26T17:33:24.830516Z",
     "start_time": "2026-01-26T17:33:24.825157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from qmap.benchmark import Sample\n",
    "import math\n",
    "def low_hemo(sample: Sample) -> bool:\n",
    "    \"\"\"Keep only samples that have a hemolytic activity (HC50) above 100 uM.\"\"\"\n",
    "    hc50 = sample.hc50.consensus\n",
    "    if math.isnan(hc50):\n",
    "        return False\n",
    "    return hc50 > 100.\n",
    "\n",
    "no_hemo_benchmark = benchmark.filter(low_hemo)\n",
    "print(no_hemo_benchmark)"
   ],
   "id": "799b4e437f4c8695",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DBAASP ID  Sequence                                 N Terminal   C Terminal   # Targets  HC50  \n",
      "8864       GILGKLWEGVKSIF                           Free         AMD          13         148.00\n",
      "22787      AVLIPFKVKFGKVKFGKVKFRCKAAFC              Free         Free         7          128.00\n",
      "1970       GAFGDLLKGVAKEAGMKLLNMAQCKLSGKC           Free         Free         8          520.00\n",
      "4588       LLIILRRRWRKQARARSK                       Free         AMD          6          200.00\n",
      "                                               ...\n",
      "14228      ILKKIWEGIKSLF                            Free         AMD          10         136.00\n",
      "3269       GMWSKILGKLIR                             Free         AMD          4          143.00\n",
      "2284       GLKEVLHSTKKFAKGFITGLTGQ                  Free         Free         2          160.00\n",
      "3275       GKWSKILGKLIR                             Free         AMD          4          200.00\n",
      "\n",
      "Total of 216 samples\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Hemolytic activity\n",
    "Similarly, you can evaluate the model on hemolytic activity by using the 'hc50' key in `compute_metrics`. You can subsample the benchmark to only keep samples that have a hemolytic activity target using the `with_hc50_targets` method."
   ],
   "id": "e7dd72570f04071"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
