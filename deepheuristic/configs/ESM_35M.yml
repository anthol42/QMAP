data:
  batch_size: [256, 256]
  num_workers: [2, 2] # local, hpc
training:
  num_epochs: 20
  lr: 0.0001
  min_lr: 0.
  weight_decay:  1.e-4
  loss: "MSE"
  optimizer: "AdamW"
model:
  model_dir: ["saved_models/ESM", ""] # TODO: Change the path for HPC
  weights_path: [".weights/esm2", ""] # TODO: Change the path for HPC
  name: "ESM_35M"
  attention_heads: 20
  embed_dim: 480
  num_layers: 12
  head_dropout: 0.
  proj_dim: 128
