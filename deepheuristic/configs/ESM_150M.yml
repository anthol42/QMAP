data:
  batch_size: [64, 64]
  num_workers: [2, 2] # local, hpc
training:
  num_epochs: 20
  lr: 1.e-5
  min_lr: 0.
  weight_decay:  1.e-4
  loss: "MSE"
  optimizer: "AdamW"
model:
  model_dir: ["saved_models/ESM", ""] # TODO: Change the path for HPC
  weights_path: [".weights/esm2", ""] # TODO: Change the path for HPC
  name: "ESM_150M"
  attention_heads: 20
  embed_dim: 640
  num_layers: 30
  head_dropout: 0.
  proj_dim: 128
