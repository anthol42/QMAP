data:
  batch_size: 128
  num_workers: [0, 2] # local, hpc
training:
  num_epochs: 5
  lr: 0.0005
  min_lr: 0.
  weight_decay:  0.
  loss: "MSE"
  optimizer: "Adam"
model:
  model_dir: ["saved_models/ESM", ""] # TODO: Change the path for HPC
  weights_path: [".weights/esm2", ""] # TODO: Change the path for HPC
  name: "ESM_8M"
  num_layers: 6
  embed_dim: 320
  attention_heads: 20
  head_dropout: 0.
  proj_dim: 128
